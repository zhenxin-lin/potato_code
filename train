# -*- coding: utf-8 -*-
"""
Created on Wed May  1 10:03:42 2019

@author: Jasmine
"""
import numpy as np
from datetime import datetime

from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential,Model
from keras.layers import Conv2D, MaxPooling2D, Input,BatchNormalization
from keras.layers import Activation, Dropout, Flatten, Dense
from keras import backend as K
from keras import applications
from keras.optimizers import Adam
from keras.utils import np_utils
import matplotlib.pyplot as plt
from keras.callbacks import Callback
from keras.callbacks import TensorBoard, ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau
from keras.callbacks import LearningRateScheduler
from keras_efficientnets import EfficientNetB4, EfficientNetB0, EfficientNetB5, EfficientNetB1, EfficientNetB2, EfficientNetB3

# dimensions of our images.
# img_width, img_height = 299, 299
img_width, img_height = 224, 224
# train_data_dir = 'E:/data6/train'
# validation_data_dir = 'E:/data6/validation'
# nb_train_samples = 2840
# nb_validation_samples = 406
# train_data_dir = 'E:/data7/train'
# validation_data_dir = 'E:/data7/validation'
# nb_train_samples = 3571
# nb_validation_samples =645
train_data_dir = 'E:/data11/train'
validation_data_dir = 'E:/data11/validation'
nb_train_samples = 6125
nb_validation_samples = 1500

model_name = "Data6EfficientB3"

class showLR( Callback ) :
    def on_epoch_begin(self, epoch, logs=None):
        lr = float(K.get_value(self.model.optimizer.lr))
        print("epoch={:02d}, lr={:.5f}".format( epoch + 1, lr ))
    
        
epochs = 1000   #trian times
batch_size = 12 #the images trains for one time

if K.image_data_format() == 'channels_first':
    input_shape = (3, img_width, img_height)
else:
    input_shape = (img_width, img_height, 3)
    
input_tensor = Input(shape=(img_width,img_height,3))

base_model = EfficientNetB3(input_tensor = input_tensor,                                      
                            include_top = False,
                            weights = 'imagenet',
                            classes = 3)
x = base_model.output
x = Flatten()(x)
x = Dense(64, activation='relu')(x)
x = Dropout(0.5)(x)
x = BatchNormalization()(x)
x = Dense(3, activation='softmax', name='predictions')(x) 

model = Model(inputs = input_tensor, outputs = x)

first_dense_idx = [index for index, layer in enumerate(model.layers) if type(layer) is Dense][0]
## train the first dense and all subsequent layers
for layer in model.layers[:first_dense_idx]:
    layer.trainable = True
for layer in model.layers[first_dense_idx:]:
    layer.trainable = True

for i, layer in enumerate(model.layers):
  print("[INFO] {}\t{}\t{:20s}\t{}".format(i, str(layer.trainable), layer.__class__.__name__, layer.name))
adam = Adam(lr=0.001)

model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])
# model.summary()
# model.load_weights("weights_best_AdagradData7EfficientNetB3_67_0.9416_0.1488.hdf5")
# Create a TensorBoard instance with the path to the logs directory
ts = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
filename = model_name + "-" + ts
tensorboard = TensorBoard(log_dir=filename,
                          histogram_freq=0,
                          write_graph=True,
                          write_images=True)

CHECKPOINT_NAME = 'weights_best_' + model_name + '_{epoch:02d}_{val_acc:.4f}_{val_loss:.4f}.hdf5'
checkpointer = ModelCheckpoint(filepath=CHECKPOINT_NAME,
                               monitor='val_loss',
                               verbose=1,
                               save_best_only=True)

early = EarlyStopping(monitor="val_loss",
                      mode="min",
                      patience=100)

show_lr = showLR()

#lr_scheduler = LearningRateScheduler(schedule_rates)

# Reduce learning rate when a metric has stopped improving
reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss',
                                   factor=0.8,
                                   patience=10,
                                   verbose=1,
                                   mode='auto',
                                   min_delta=0.0001,
                                   cooldown=3,
                                   min_lr=0.00001)
    
#callbacks = [lr_scheduler, checkpointer, tensorboard, early, reduceLROnPlat, show_lr]
callbacks = [checkpointer, tensorboard, early, reduceLROnPlat, show_lr]
    
# this is the augmentation configuration we will use for training
train_datagen = ImageDataGenerator(
    featurewise_center = True,
    featurewise_std_normalization = True,
    rescale=1. / 255,
    shear_range = 0.2,
    zoom_range = 0.2,
    horizontal_flip=True,
    vertical_flip=True,
    width_shift_range = 0.1,
    height_shift_range = 0.1,
    brightness_range = [0.75, 1.25],
    channel_shift_range = 0.1,
    rotation_range = 180)

# this is the augmentation configuration we will use for testing:
# only rescaling
test_datagen = ImageDataGenerator(rescale=1. / 255)

train_generator = train_datagen.flow_from_directory(
    train_data_dir,
    target_size=(img_width, img_height),
    batch_size=batch_size,
    class_mode='categorical')

validation_generator = test_datagen.flow_from_directory(
    validation_data_dir,
    target_size=(img_width, img_height),
    batch_size=batch_size,
    class_mode='categorical')

train_log = model.fit_generator(
    train_generator,
    steps_per_epoch=nb_train_samples // batch_size,
    epochs=epochs,
    validation_data=validation_generator,
    validation_steps=nb_validation_samples // batch_size,
    callbacks=callbacks)
